{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import matplotlib\n",
    "\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "matplotlib.rcParams.update({\n",
    "        \"font.family\": \"serif\",\n",
    "       \"font.serif\": [\"DejaVu Serif\", \"Bitstream Vera Serif\", \"Computer Modern Roman\", \"New Century Schoolbook\", \"Century Schoolbook L\", \"Utopia\", \"ITC Bookman\", \"Bookman\", \"Nimbus Roman No9 L\", \"Times New Roman\", \"Times\", \"Palatino\", \"Charter\", \"serif\"],\n",
    "        \"axes.labelsize\": 18,\n",
    "        \"font.size\": 18,\n",
    "        \"legend.fontsize\": 16,\n",
    "        \"xtick.labelsize\": 18,\n",
    "        \"ytick.labelsize\": 18,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sgd_mom(W_0, data, b, lr=0.02, beta=0.9, n_iter=10000):\n",
    "    W = W_0.clone().detach().requires_grad_(True)\n",
    "    m, n = W.size()\n",
    "    mom = torch.zeros(m, n)\n",
    "    total_loss = ((W @ data)**2).sum() / n\n",
    "    losses = [total_loss.item()]\n",
    "    for t in range(1, n_iter+1):\n",
    "        i = np.random.choice(n, (b,), replace=False)\n",
    "        loss = ((W @ data[:, i])**2).sum() / b\n",
    "        loss.backward()\n",
    "        with torch.no_grad():\n",
    "            mom = beta * mom + (1 - beta) * W.grad\n",
    "            W -= lr * mom / (1 - beta**t)\n",
    "            W.grad.zero_()\n",
    "        total_loss = ((W @ data)**2).sum() / n\n",
    "        losses.append(total_loss.item())\n",
    "    return W, losses\n",
    "\n",
    "def sgd_mom_galore(W_0, data, b, r, lr=0.1, beta=0.9, n_iter=10000, T=10):\n",
    "    W = W_0.clone().detach().requires_grad_(True)\n",
    "    m, n = W.size()\n",
    "    U = None\n",
    "    mom = torch.zeros(r, n)\n",
    "    total_loss = ((W @ data)**2).sum() / n\n",
    "    losses = [total_loss.item()]\n",
    "    for t in range(1, n_iter+1):\n",
    "        i = np.random.choice(n, (b,), replace=False)\n",
    "        loss = ((W @ data[:, i])**2).sum() / b\n",
    "        loss.backward()\n",
    "        with torch.no_grad():\n",
    "            if t % T == 1 or T == 1:\n",
    "                U, _, _ = torch.svd(W.grad)\n",
    "                U = U[:, :r]\n",
    "            mom = beta * mom + (1 - beta) * (U.T @ W.grad)\n",
    "            W -= lr / (1 - beta**t) * U @ mom \n",
    "            W.grad.zero_()\n",
    "        total_loss = ((W @ data)**2).sum() / n\n",
    "        losses.append(total_loss.item())\n",
    "    return W, losses\n",
    "\n",
    "def sgd_mom_proper_galore(W_0, data, b, r, lr=0.1, beta=0.9, n_iter=10000, T=10):\n",
    "    W = W_0.clone().detach().requires_grad_(True)\n",
    "    m, n = W.size()\n",
    "    U = None\n",
    "    mass = 0\n",
    "    mom = torch.zeros(r, n)\n",
    "    total_loss = ((W @ data)**2).sum() / n\n",
    "    losses = [total_loss.item()]\n",
    "    for t in range(1, n_iter+1):\n",
    "        batch_i = np.random.choice(n, (b,), replace=False)\n",
    "        loss = ((W @ data[:, batch_i])**2).sum() / b\n",
    "        loss.backward()\n",
    "        with torch.no_grad():\n",
    "            if t % T == 1 or T == 1:\n",
    "                old_mom = mom.clone()\n",
    "                if t > 1:\n",
    "                    mom = U @ mom # lifting to original space\n",
    "                U, _, _ = torch.svd(W.grad)\n",
    "                U = U[:, :r]\n",
    "                if t > 1:\n",
    "                    mom = U.T @ mom # projection to new basis\n",
    "                # recalibrate mass\n",
    "                if t > 1:\n",
    "                    mass *=  mom.norm()/ old_mom.norm()\n",
    "            mom = beta * mom + (1 - beta) * (U.T @ W.grad)\n",
    "            mass = beta * mass + (1 - beta) * 1\n",
    "            mom_hat = lr * mom / mass\n",
    "            W -= U @ mom_hat\n",
    "            W.grad.zero_()\n",
    "        total_loss = ((W @ data)**2).sum() / n\n",
    "        losses.append(total_loss.item())\n",
    "    return W, losses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "b = 2\n",
    "# r = 1\n",
    "n_iter = 200\n",
    "T = 10\n",
    "\n",
    "m, n = 10, 10\n",
    "seed = 1\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "W_0 = torch.randn(m, n)  # initialize W randomly\n",
    "\n",
    "m, n = W_0.size()\n",
    "data = torch.eye(n)\n",
    "\n",
    "lr = 0.1\n",
    "\n",
    "plt.figure(figsize=(20, 15))\n",
    "\n",
    "for r in [3, 6]:    \n",
    "    # 5 repetitions for averaging\n",
    "    \n",
    "    losses_sgd_galore = []\n",
    "    losses_sgd_proper_galore = []\n",
    "\n",
    "    for _ in range(5):\n",
    "        _, loss = sgd_mom_galore(W_0, data, b, r, n_iter = n_iter, T = T, lr=lr)\n",
    "        losses_sgd_galore.append(loss)\n",
    "        _, loss = sgd_mom_proper_galore(W_0, data, b, r, n_iter = n_iter, T = T, lr=lr)\n",
    "        losses_sgd_proper_galore.append(loss)\n",
    "\n",
    "    # plot average and std\n",
    "    losses_sgd_galore = np.array(losses_sgd_galore)\n",
    "    losses_sgd_proper_galore = np.array(losses_sgd_proper_galore)\n",
    "\n",
    "    plt.plot(losses_sgd_galore.mean(0), label=f'GaLore-like SGDM, rank {r}')\n",
    "    plt.fill_between(np.arange(n_iter+1), losses_sgd_galore.mean(0) - losses_sgd_galore.std(0), losses_sgd_galore.mean(0) + losses_sgd_galore.std(0), alpha=0.2)\n",
    "    plt.plot(losses_sgd_proper_galore.mean(0), label=f' GaLore-like SGDM with Momentum re-projection, rank {r}')\n",
    "    plt.fill_between(np.arange(n_iter+1), losses_sgd_proper_galore.mean(0) - losses_sgd_proper_galore.std(0), losses_sgd_proper_galore.mean(0) + losses_sgd_proper_galore.std(0), alpha=0.2)\n",
    "\n",
    "plt.legend(fontsize=24)\n",
    "plt.xlabel('Iteration', fontsize=32)\n",
    "plt.ylabel('Loss', fontsize=32)\n",
    "plt.yscale('log')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'reproj_toy.pdf', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
